# load dependencies and libraries
library(tidyverse)
library(readr)
library(keras)
library(caret)

setwd("~/") #sets working directory
input_path <- "abc" #file path

folder <- "xyz" #treatment

trace_table <- read_csv(paste0("/",folder,"/",folder,".csv"))
trace_table2B <- t(scale(trace_table[,-c(1:2)]))
trace_table2 <- trace_table2B[,1:13]

clusters <- read_csv(paste0(input_path,"/",folder,"/post/",folder,"_post_clusters.csv"))
final_accuracy <- as.data.frame(matrix(0,nrow=300,ncol=3))
colnames(final_accuracy)<-c("test_accuracy","train_accuracy","random_test_accuracy")

clusters_ranked <- as.data.frame(rownames(trace_table2))
for(j in 1:nrow(clusters_ranked)){
  rn <- filter(clusters,clusters$`rownames(mycl2)`==clusters_ranked[j,1])
  clusters_ranked[j,2]<-rn[,3]
}

trace_table3 <- as.data.frame(cbind(clusters_ranked[,2],trace_table2))
trace_table3$V1=trace_table3$V1-1
cluster_response <- trace_table3[,1]

#5-fold cross validation
folds <- createFolds(trace_table3[,1], k = 5) # c
if (!is.numeric(cluster_response)) {
  cluster_response <- as.numeric(as.factor(cluster_response))
}
trace_table3[,1]<-NULL

# Store performance metrics
accuracy <- as.data.frame(numeric(length(folds)))

for(i in seq_along(folds)) {
  # Split the data
  test_indices <- folds[[i]]
  x_train <- as.matrix(trace_table3[-test_indices, ])
  y_train <- to_categorical(cluster_response[-test_indices], num_classes = max(clusters$mycl))
  x_test <- as.matrix(trace_table3[test_indices, ])
  y_test <- to_categorical(cluster_response[test_indices], num_classes = max(clusters$mycl))
  
  # Build and compile the model (same as your original model)
  model <- keras_model_sequential() %>%
    layer_dense(units = 64, activation = 'relu', input_shape = dim(x_train)[2],kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01)) %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = max(clusters$mycl), activation = 'softmax')
  
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(0.01),
    metrics = c('accuracy')
  )
  
  # Fit the model
  history <- model %>% fit(
    x_train,
    y_train,
    epochs = 10,
    batch_size = 64
  )
  
  # Evaluate the model
  metrics <- model %>% evaluate(x_test, y_test)
  accuracy[i,1] <- metrics[2]
  metrics2 <- model %>% evaluate(x_train, y_train)
  accuracy[i,2] <- metrics2[2]
}

# Average accuracy over all folds
mean_accuracy <- colMeans(accuracy)

# scrambled response labels and repeat the analyses
clusters_ranked <- as.data.frame(rownames(trace_table2))
for(j in 1:nrow(clusters_ranked)){
  rn <- filter(clusters,clusters$`rownames(mycl2)`==clusters_ranked[j,1])
  clusters_ranked[j,2]<-rn[,3]
}

clusters_ranked[,2]<-clusters_ranked[sample(nrow(clusters_ranked)),2]
trace_table3 <- as.data.frame(cbind(clusters_ranked[,2],trace_table2))
trace_table3$V1=trace_table3$V1-1
cluster_response <- trace_table3[,1]

folds <- createFolds(trace_table3[,1], k = 5) # c
if (!is.numeric(cluster_response)) {
  cluster_response <- as.numeric(as.factor(cluster_response))
}
trace_table3[,1]<-NULL

accuracy <- as.data.frame(numeric(length(folds)))

for(i in seq_along(folds)) {
  # Split the data
  test_indices <- folds[[i]]
  x_train <- as.matrix(trace_table3[-test_indices, ])
  y_train <- to_categorical(cluster_response[-test_indices], num_classes = max(clusters$mycl))
  x_test <- as.matrix(trace_table3[test_indices, ])
  y_test <- to_categorical(cluster_response[test_indices], num_classes = max(clusters$mycl))
  
  # Build and compile the model (same as your original model)
  model <- keras_model_sequential() %>%
    layer_dense(units = 64, activation = 'relu', input_shape = dim(x_train)[2],kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01)) %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = max(clusters$mycl), activation = 'softmax')
  
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(0.01),
    metrics = c('accuracy')
  )
  
  # Fit the model
  history <- model %>% fit(
    x_train,
    y_train,
    epochs = 10,
    batch_size = 64
  )
  
  # Evaluate the model
  metrics <- model %>% evaluate(x_test, y_test)
  accuracy[i,] <- metrics[2]
}

# Average accuracy over all folds
mean_accuracy2 <- mean(accuracy$`numeric(length(folds))`)
final_accuracy[rpt,1:3]<-c(mean_accuracy,mean_accuracy2)
write.csv(final_accuracy, paste0(input_path,"/accuracy_",folder,".csv"))



